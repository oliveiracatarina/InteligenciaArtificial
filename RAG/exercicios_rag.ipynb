{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244117c0",
   "metadata": {},
   "source": [
    "# Exercício 1 – Teórico: Fundamentos de RAG\n",
    "\n",
    "Explique, com suas próprias palavras:\n",
    "\n",
    "1. O que é RAG (Retrieval-Augmented Generation) e qual problema ele resolve na geração de texto usando modelos de linguagem?\n",
    "2. Quais são as vantagens de utilizar RAG em vez de depender apenas do modelo base?\n",
    "3. Quais são as principais etapas de um pipeline de RAG? Explique brevemente cada uma das seguintes fases:\n",
    "- Chunking\n",
    "- Indexing\n",
    "- Retrieval\n",
    "- Reranking\n",
    "- Geração (Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0badfd",
   "metadata": {},
   "source": [
    "# Exercício 2 – Prático: Chunking de Documentos\n",
    "\n",
    "Implemente uma função que realiza o *chunking* de um documento de texto longo.\n",
    "\n",
    "Requisitos:\n",
    "- A função deve dividir o texto em pedaços (chunks) de tamanho configurável (ex.: 500 tokens ou 1000 caracteres).\n",
    "- Deve permitir uma sobreposição (*overlap*) entre os chunks (ex.: 20% de sobreposição).\n",
    "- O output esperado é uma lista de chunks.\n",
    "\n",
    "Exemplo de uso esperado:\n",
    "```python\n",
    "chunks = chunk_text(long_text, chunk_size=500, overlap=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75856b2",
   "metadata": {},
   "source": [
    "# Exercício 3 – Prático: Indexação Vetorial\n",
    "\n",
    "Com os chunks criados no exercício anterior:\n",
    "\n",
    "1. Gere embeddings para cada chunk usando um modelo de embeddings (ex.: `sentence-transformers`, OpenAI embeddings ou outro de sua escolha).\n",
    "2. Crie um índice vetorial (FAISS, ChromaDB, Elasticsearch, etc.).\n",
    "3. Implemente uma função para realizar buscas no índice, retornando os Top-K documentos mais similares a uma query.\n",
    "\n",
    "Exemplo de uso esperado:\n",
    "```python\n",
    "results = search_index(query=\"Qual o impacto da inflação?\", top_k=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d365611",
   "metadata": {},
   "source": [
    "# Exercício 4 – Prático: Recuperação de Contexto + Geração de Resposta\n",
    "\n",
    "Implemente uma função que faça o seguinte:\n",
    "\n",
    "1. Receba uma query do usuário.\n",
    "2. Recupere os Top-K chunks mais relevantes do índice vetorial.\n",
    "3. Monte um *prompt* contendo a query + os textos dos chunks recuperados.\n",
    "4. Envie o prompt para um LLM (ex.: OpenAI, Mistral, Llama ou outro) para gerar uma resposta.\n",
    "\n",
    "Exemplo de uso esperado:\n",
    "```python\n",
    "response = rag_pipeline(query=\"Explique o conceito de inflação\")\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8611a",
   "metadata": {},
   "source": [
    "# Exercício 5 – Prático: Reranking\n",
    "\n",
    "Implemente uma etapa de reranking para melhorar a qualidade dos documentos recuperados:\n",
    "\n",
    "1. Após o retrieval inicial (ex.: Top-10), use um modelo de reranking (ex.: um `cross-encoder` da `sentence-transformers`) para reordenar os documentos com base na relevância para a query.\n",
    "2. Compare as respostas geradas pelo RAG antes e depois do reranking.\n",
    "3. Discuta brevemente se o reranking trouxe melhorias na qualidade da resposta.\n",
    "\n",
    "Exemplo de uso esperado:\n",
    "```python\n",
    "ranked_results = rerank(query, retrieved_docs)\n",
    "response = generate_answer(query, ranked_results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88b444",
   "metadata": {},
   "source": [
    "## Import and Install\n",
    "Todos os imports e %pip install que foram usados nos exercícios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c81449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Pip Install\n",
    "%pip install sentence-transformers faiss-cpu --quiet\n",
    "%pip install openai --quiet\n",
    "\n",
    "# Set OpenAI API key \n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247a111",
   "metadata": {},
   "source": [
    "## Resposta - Exercício 1\n",
    "\n",
    "1. **O que é RAG (Retrieval-Augmented Generation) e qual problema ele resolve na geração de texto usando modelos de linguagem?**  \n",
    "RAG é uma técnica que combina modelos de linguagem (LLMs) com mecanismos de busca em bases de dados externas. O objetivo é permitir que o modelo acesse informações atualizadas ou específicas durante a geração de texto, superando a limitação dos LLMs tradicionais, que só usam o conhecimento aprendido até o momento do treinamento. Assim, o RAG resolve o problema de respostas desatualizadas ou incompletas, tornando o modelo capaz de citar fontes e trazer dados recentes.\n",
    "\n",
    "2. **Vantagens de utilizar RAG em vez de depender apenas do modelo base:**  \n",
    "- Permite respostas mais precisas e atualizadas, pois consulta fontes externas.\n",
    "- Reduz a alucinação (respostas inventadas) dos modelos.\n",
    "- Possibilita citar fontes e justificar respostas.\n",
    "- Facilita a adaptação para domínios específicos, usando bases de dados customizadas.\n",
    "\n",
    "3. **Principais etapas de um pipeline de RAG:**\n",
    "- **Chunking:** Dividir documentos em partes menores (chunks) para facilitar a busca e o processamento.\n",
    "- **Indexing:** Criar um índice vetorial dos chunks, permitindo buscas rápidas por similaridade semântica.\n",
    "- **Retrieval:** Buscar os chunks mais relevantes para uma consulta do usuário.\n",
    "- **Reranking:** Reordenar os resultados recuperados, priorizando os mais relevantes para a query.\n",
    "- **Geração (Generation):** Usar o modelo de linguagem para gerar uma resposta, combinando a query do usuário com os chunks recuperados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd885a6d",
   "metadata": {},
   "source": [
    "## Resposta - Exercício 2\n",
    "- A função `chunk_text` divide um texto longo em pedaços (chunks) de tamanho configurável (`chunk_size`).\n",
    "\n",
    "- Permite uma sobreposição entre os chunks, controlada pelo parâmetro `overlap`.\n",
    "\n",
    "- O output é uma lista de chunks, como solicitado.\n",
    "\n",
    "- O exemplo de uso mostra como utilizar a função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b56b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 8\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into small...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "hunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "ller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be spli...\n",
      "\n",
      "--- Chunk 4 ---\n",
      "to smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will b...\n",
      "\n",
      "--- Chunk 5 ---\n",
      "lit into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that ...\n",
      "\n",
      "--- Chunk 6 ---\n",
      " be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text...\n",
      "\n",
      "--- Chunk 7 ---\n",
      "t will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a lon...\n",
      "\n",
      "--- Chunk 8 ---\n",
      "xt that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of a long text that will be split into smaller chunks.Example of...\n"
     ]
    }
   ],
   "source": [
    "# Example long text\n",
    "long_text = \"Example of a long text that will be split into smaller chunks.\" * 100  # Simulating a long text\n",
    "\n",
    "# Function to split a long text into chunks with overlap\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    while start < text_length:\n",
    "        end = min(start + chunk_size, text_length)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        # Move to the next chunk, considering the overlap\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Applying the function\n",
    "chunks = chunk_text(long_text)\n",
    "\n",
    "# Visualization of the result\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk[:300]}...\")  #mostra os primeiros 300 caracteres de cada parte\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# long_text = \"...\" \n",
    "# chunks = chunk_text(long_text, chunk_size=1000, overlap=200)\n",
    "# print(f\"Number of chunks: {len(chunks)}\")\n",
    "# print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef403d2f",
   "metadata": {},
   "source": [
    "## Resposta - Exercício 3\n",
    "- Gera embeddings para cada chunk usando um modelo de sentence transformer.\n",
    "\n",
    "- Cria um índice FAISS para busca vetorial rápida.\n",
    "\n",
    "- Implementa uma função para buscar os chunks mais similares a uma query.\n",
    "\n",
    "- Retorna os top-k chunks mais similares e seus scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51e47bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar results to the query:\n",
      "\n",
      "--- Result 1 ---\n",
      "Chunk: Inflation is the general increase in prices over time.\n",
      "Score (lower is more similar): 0.5827\n",
      "\n",
      "--- Result 2 ---\n",
      "Chunk: High inflation can erode purchasing power.\n",
      "Score (lower is more similar): 0.6654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Example: list of text chunks (replace with your actual chunks)\n",
    "chunks = [\n",
    "    \"Inflation is the general increase in prices over time.\",\n",
    "    \"Central banks use interest rates to control inflation.\",\n",
    "    \"High inflation can erode purchasing power.\",\n",
    "    \"Deflation is the opposite of inflation.\"\n",
    "]\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "# Create a FAISS index for fast similarity search\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Function to search the index and return top-k most similar chunks\n",
    "def search_index(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the most similar chunks to the query.\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        top_k (int): Number of top results to return.\n",
    "    Returns:\n",
    "        List of (chunk, score) tuples.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        results.append((chunks[idx], dist))\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "results = search_index(\"What is inflation?\", top_k=2)\n",
    "print(\"Most similar results to the query:\\n\")\n",
    "for i, (chunk, score) in enumerate(results, 1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(f\"Chunk: {chunk}\")\n",
    "    print(f\"Score (lower is more similar): {score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60529fbf",
   "metadata": {},
   "source": [
    "## Resposta - Exercício 4\n",
    "- Receba uma query do usuário.\n",
    "- Recupere os Top-K chunks mais relevantes do índice vetorial.\n",
    "- Monte um *prompt* contendo a query + os textos dos chunks recuperados.\n",
    "- Envie o prompt para um LLM (ex.: OpenAI, Mistral, Llama ou outro) para gerar uma resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8418e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG pipeline function: Retrieve context and generate answer using OpenAI\n",
    "import openai\n",
    "\n",
    "def rag_pipeline(query, chunks, model_name=\"gpt-3.5-turbo\", top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant chunks and generates an answer with OpenAI LLM.\n",
    "    Args:\n",
    "        query (str): User question.\n",
    "        chunks (list): List of text chunks.\n",
    "        model_name (str): OpenAI model name.\n",
    "        top_k (int): How many chunks to use as context.\n",
    "    Returns:\n",
    "        str: Answer generated by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embeddings for the chunks and the query\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    chunk_embs = model.encode(chunks)\n",
    "    query_emb = model.encode([query])\n",
    "\n",
    "    # Compute similarity (dot product)\n",
    "    sims = np.dot(chunk_embs, query_emb[0])\n",
    "    # Get indices of top_k most similar chunks\n",
    "    top_indices = np.argsort(sims)[-top_k:][::-1]\n",
    "    top_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "    # Build prompt with context\n",
    "    context = \"\\n\".join(top_chunks)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    # Generate answer using OpenAI (new API >=1.0.0)\n",
    "    response = openai.resources.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Example usage of the RAG pipeline\n",
    "# Use the already defined 'chunks' variable from previous cells\n",
    "test_query = \"Explain the concept of inflation\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6b380",
   "metadata": {},
   "source": [
    "## Resposta - Exercício 5\n",
    "- Após o retrieval inicial (ex.: Top-10), use um modelo de reranking (ex.: um `cross-encoder` da `sentence-transformers`) para reordenar os documentos com base na relevância para a query.\n",
    "- Compare as respostas geradas pelo RAG antes e depois do reranking.\n",
    "- Discuta brevemente se o reranking trouxe melhorias na qualidade da resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "950aea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the concept of inflation\n",
      "\n",
      "Retrieved documents:\n",
      "--- Doc 1 ---\n",
      "Inflation is the general increase in prices over time.\n",
      "\n",
      "--- Doc 2 ---\n",
      "Central banks use interest rates to control inflation.\n",
      "\n",
      "--- Doc 3 ---\n",
      "High inflation can erode purchasing power.\n",
      "\n",
      "--- Doc 4 ---\n",
      "Deflation is the opposite of inflation.\n",
      "\n",
      "Documents after reranking:\n",
      "--- Doc 1 (score=5.0835) ---\n",
      "Inflation is the general increase in prices over time.\n",
      "\n",
      "--- Doc 2 (score=-2.7351) ---\n",
      "Deflation is the opposite of inflation.\n",
      "\n",
      "--- Doc 3 (score=-6.8251) ---\n",
      "High inflation can erode purchasing power.\n",
      "\n",
      "--- Doc 4 (score=-9.9385) ---\n",
      "Central banks use interest rates to control inflation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reranking function using sentence-transformers cross-encoder\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(query, retrieved_docs, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'):\n",
    "    \"\"\"\n",
    "    Reorders retrieved documents based on relevance to the query using a cross-encoder.\n",
    "    Args:\n",
    "        query (str): User query.\n",
    "        retrieved_docs (list): List of retrieved texts.\n",
    "        model_name (str): Cross-encoder model name.\n",
    "    Returns:\n",
    "        list: List of (doc, score) sorted by relevance.\n",
    "    \"\"\"\n",
    "    cross_encoder = CrossEncoder(model_name)\n",
    "    pairs = [[query, doc] for doc in retrieved_docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    ranked = sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked\n",
    "\n",
    "# Function to generate final answer using the reranked documents\n",
    "import openai\n",
    "\n",
    "def generate_answer(query, ranked_docs, model_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Generates an answer using the reranked documents as context.\n",
    "    Args:\n",
    "        query (str): User query.\n",
    "        ranked_docs (list): List of (doc, score) sorted by relevance.\n",
    "        model_name (str): OpenAI model name.\n",
    "    Returns:\n",
    "        str: Answer generated by the model.\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join([doc for doc, _ in ranked_docs])\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Visualization of the pipeline with reranking (Exercise 5)\n",
    "\n",
    "# Example of retrieved documents (replace with your actual chunks)\n",
    "retrieved_docs = [\n",
    "    \"Inflation is the general increase in prices over time.\",\n",
    "    \"Central banks use interest rates to control inflation.\",\n",
    "    \"High inflation can erode purchasing power.\",\n",
    "    \"Deflation is the opposite of inflation.\"\n",
    "]\n",
    "\n",
    "query = \"Explain the concept of inflation\"\n",
    "\n",
    "# Reranking the documents\n",
    "ranked_results = rerank(query, retrieved_docs)\n",
    "\n",
    "# Visualization of results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"--- Doc {i} ---\\n{doc}\\n\")\n",
    "\n",
    "print(\"Documents after reranking:\")\n",
    "for i, (doc, score) in enumerate(ranked_results, 1):\n",
    "    print(f\"--- Doc {i} (score={score:.4f}) ---\\n{doc}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Exemplo de uso:\n",
    "# retrieved_docs = [\"Texto 1...\", \"Texto 2...\", ...]\n",
    "# ranked_results = rerank(\"Explique o conceito de inflação\", retrieved_docs)\n",
    "# resposta = generate_answer(\"Explique o conceito de inflação\", ranked_results)\n",
    "# print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
