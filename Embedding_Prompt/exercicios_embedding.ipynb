{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc78e995",
   "metadata": {},
   "source": [
    "# Seção 1 - Exercícios Teóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25f8bd",
   "metadata": {},
   "source": [
    "\n",
    "## **Exercício 1: Teórico - Entendendo Embeddings de Palavras**\n",
    "**Objetivo:** Explicar o conceito de embeddings de palavras.\n",
    "**Instruções:**\n",
    "- Descreva o que é um embedding de palavra e como ele difere de métodos tradicionais como codificação one-hot.\n",
    "- Discuta as vantagens de usar embeddings.\n",
    "- Explique como a similaridade cosseno é usada para comparar embeddings.\\\n",
    "\n",
    "### Resposta:\n",
    "**O que é um embedding de palavra?**  \n",
    "Um embedding de palavra é uma representação vetorial densa de palavras em um espaço contínuo de alta dimensão, onde palavras com significados semelhantes estão próximas umas das outras. Diferente da codificação one-hot, que representa cada palavra como um vetor esparso com apenas um elemento igual a 1 e o restante 0, os embeddings capturam relações semânticas e sintáticas entre palavras, permitindo que o modelo compreenda similaridades e contextos.\n",
    "\n",
    "**Vantagens de usar embeddings:**  \n",
    "- Capturam similaridades semânticas e sintáticas entre palavras.\n",
    "- Reduzem a dimensionalidade em relação ao one-hot, tornando o processamento mais eficiente.\n",
    "- Permitem que modelos de NLP generalizem melhor para palavras e contextos não vistos durante o treinamento.\n",
    "\n",
    "**Similaridade cosseno:**  \n",
    "A similaridade cosseno é uma métrica usada para medir o quão semelhantes dois vetores são, considerando apenas a direção e não o módulo. No contexto de embeddings, ela é usada para comparar o quão próximas semanticamente duas palavras estão: quanto mais próximo de 1, mais similares; quanto mais próximo de 0, menos relacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95091827",
   "metadata": {},
   "source": [
    "## **Exercício 2: Teórico - Codificação Posicional em Transformers**\n",
    "**Objetivo:** Entender a importância da codificação posicional e suas implicações nos modelos de linguagem.\n",
    "**Instruções:**\n",
    "1. Explique por que a codificação posicional é necessária em Transformers.\n",
    "2. Descreva a diferença entre codificações posicionais fixas (baseadas em funções seno e cosseno) e codificações aprendidas.\n",
    "3. Como a codificação posicional interage com o mecanismo de atenção nos Transformers?\n",
    "\n",
    "### Resposta:\n",
    "1. **Por que a codificação posicional é necessária em Transformers?**  \n",
    "Transformers não possuem uma estrutura sequencial explícita como RNNs ou CNNs, ou seja, eles processam todos os tokens de uma sequência em paralelo e não têm noção da ordem dos elementos. A codificação posicional é necessária para informar ao modelo a posição de cada token na sequência, permitindo que ele capture relações dependentes da ordem das palavras.\n",
    "\n",
    "2. **Diferença entre codificações posicionais fixas e aprendidas:**  \n",
    "- **Fixas (seno e cosseno):** Utilizam funções trigonométricas para gerar vetores de posição, de forma determinística e sem parâmetros treináveis. Cada posição tem um vetor único, e o padrão é sempre o mesmo, independente dos dados.\n",
    "- **Aprendidas:** Os vetores de posição são parâmetros treináveis do modelo, aprendidos durante o treinamento. Isso permite que o modelo adapte as representações posicionais conforme o contexto dos dados.\n",
    "\n",
    "3. **Como a codificação posicional interage com o mecanismo de atenção nos Transformers?**  \n",
    "A codificação posicional é somada (ou concatenada) aos embeddings das palavras antes de serem processados pelas camadas de atenção. Assim, o mecanismo de atenção pode considerar tanto o conteúdo das palavras quanto suas posições relativas na sequência, permitindo que o modelo aprenda dependências que envolvem a ordem dos tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066f409",
   "metadata": {},
   "source": [
    "## **Exercício 3: Teórico - Introdução à Engenharia de Prompt**\n",
    "**Objetivo:** Entender o conceito e técnicas de engenharia de prompt.\n",
    "**Instruções:**\n",
    "- Defina engenharia de prompt com suas próprias palavras.\n",
    "- Explique por que engenharia de prompt é crítica ao trabalhar com LLMs.\n",
    "- Liste e descreva brevemente pelo menos três técnicas: zero-shot, few-shot, chain-of-thought.\n",
    "\n",
    "### Resposta:\n",
    "**O que é engenharia de prompt?**  \n",
    "Engenharia de prompt é a prática de criar, ajustar e otimizar instruções (prompts) para direcionar modelos de linguagem (LLMs) a produzirem respostas mais precisas, relevantes ou úteis para uma determinada tarefa.\n",
    "\n",
    "**Por que engenharia de prompt é crítica ao trabalhar com LLMs?**  \n",
    "Porque a forma como o prompt é escrito influencia diretamente a qualidade, clareza e utilidade da resposta do modelo. Prompts bem elaborados ajudam o modelo a entender melhor o contexto e a intenção do usuário, reduzindo ambiguidades e aumentando a eficiência do uso do LLM.\n",
    "\n",
    "**Três técnicas de engenharia de prompt:**\n",
    "\n",
    "- **Zero-shot:**  \n",
    "  O modelo recebe apenas a instrução da tarefa, sem exemplos. Exemplo: \"Traduza para o inglês: casa.\"\n",
    "\n",
    "- **Few-shot:**  \n",
    "  O prompt inclui alguns exemplos de entrada e saída antes da tarefa principal, ajudando o modelo a entender o padrão esperado. Exemplo:  \n",
    "  \"Português: casa → Inglês: house  \n",
    "  Português: gato → Inglês: cat  \n",
    "  Português: livro → Inglês: book  \n",
    "  Português: árvore → Inglês:\"\n",
    "\n",
    "- **Chain-of-thought:**  \n",
    "  O prompt incentiva o modelo a mostrar o raciocínio passo a passo antes de chegar à resposta final, útil para tarefas de lógica ou matemática. Exemplo:  \n",
    "  \"Maria tem 3 maçãs e ganha mais 2. Quantas maçãs ela tem agora? Vamos pensar passo a passo.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8af8d",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
